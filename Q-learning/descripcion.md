# Algoritmo Q-Learning
## Configuraci√≥n Inicial: El Entorno y la Q-Tabla
### Paso 1: Entorno y Espacio de Estados Continuo (MountainCar)
En el entorno MountainCar-v0 el estado del coche se describe con dos n√∫meros continuos (decimales):

- Posici√≥n: Un valor entre $-1.2$ y $0.6$.
- Velocidad: Un valor entre $-0.07$ y $0.07$

Como Q-Learning necesita estados discretos (enteros) para indexar una tabla, no podemos usar la posici√≥n y la velocidad directamente

### Paso 2: La Discretizaci√≥n de Estados (El Mapeo) üó∫Ô∏è
Para que el Q-Learning funcione, debes discretizar el espacio de estados. Esto significa dividir el rango continuo de la posici√≥n y la velocidad en un n√∫mero finito de bins.

**ESTRO DESCRIBIRLO CON LOS VALORES QUE USEMOS:** Si decides usar 20 bins para la posici√≥n y 20 para la velocidad, tendr√≠as un total de $20 \times 20 = 400$ estados discretos.Una posici√≥n real como $-0.5532$ podr√≠a mapearse al bin de posici√≥n n√∫mero 5, y una velocidad de $0.0211$ al bin de velocidad n√∫mero 12.El estado discreto es ahora el par de √≠ndices $(5, 12)$.

### Paso 3: Inicializaci√≥n de la Q-Tabla
La Q-Tabla es la "memoria" del agente. Es una tabla tridimensional donde se almacenan los valores de "calidad" o recompensa esperada.

- Dimensiones: (Bins de Posici√≥n) $\times$ (Bins de Velocidad) $\times$ (N√∫mero de Acciones).
- Para MountainCar: $N_{bins} \times N_{bins} \times 3$ (Las 3 acciones son: empujar izquierda, no hacer nada, empujar derecha).
- Contenido Inicial: Todos los valores $Q(s, a)$ se inicializan a cero o a un n√∫mero peque√±o y aleatorio. Esto significa que al principio, el agente piensa que todas las acciones son igual de in√∫tiles (o √∫tiles) en todas las situaciones.

## El Bucle de Entrenamiento: Episodios y Pasos
El entrenamiento ocurre a lo largo de muchos episodios. Un episodio comienza con el coche en la posici√≥n inicial y termina cuando alcanza la bandera (meta) o el n√∫mero m√°ximo de pasos.

### Paso 4: Selecci√≥n de la Acci√≥n ($\epsilon$-Greedy)
En cada paso del episodio, el agente debe decidir qu√© acci√≥n tomar:
- **Exploraci√≥n** (Probabilidad $\epsilon$): El agente elige una acci√≥n al azar. Esto es crucial para descubrir nuevas estrategias que podr√≠an ser mejores.
- **Explotaci√≥n** (Probabilidad $1-\epsilon$): El agente consulta la Q-Tabla para el estado actual $(i_{pos}, i_{vel})$ y elige la acci√≥n con el valor Q m√°s alto (la que ha sido m√°s exitosa en el pasado).

Al inicio, $\epsilon$ es alto (ej. 1.0), por lo que el agente explora mucho. Con el tiempo, $\epsilon$ decae, y el agente explota (usa lo que ha aprendido) cada vez m√°s.

### Paso 5: Interacci√≥n con el Entorno
El agente ejecuta la acci√≥n seleccionada ($a$) en el estado actual ($s$). El entorno devuelve:

1. Un nuevo estado ($s'$).
2. Una recompensa ($R$): Para MountainCar, es $-1$ por cada paso (castigo) y $0$ si alcanza la meta.
3. Si el episodio ha terminado (done).

### Paso 6: El Coraz√≥n del Q-Learning (Actualizaci√≥n de la Q-Tabla)
Este es el paso fundamental. El agente usa la informaci√≥n que acaba de recibir ($s, a, R, s'$) para actualizar el valor $Q(s, a)$ usando la Ecuaci√≥n de Bellman para Q-Learning:
$$\text{Nuevo } Q(s, a) \leftarrow Q(s, a) + \alpha \left[ R + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]$$
1. **El Error Temporal (TD Error):** Lo que est√° entre corchetes es el error de predicci√≥n (qu√© tan equivocado estaba el valor Q anterior).
    - **Lo Esperado:** $R + \gamma \max_{a'} Q(s', a')$
        - **$R$:** La recompensa inmediata recibida.
        - **$\gamma \max_{a'} Q(s', a')$:** La mejor recompensa futura que se puede obtener desde el nuevo estado $s'$ (el valor Q m√°s alto de las acciones posibles en $s'$). Esta es la parte off-policy.
    - **Lo Predicho:** $Q(s, a)$ (el valor Q que ten√≠as antes de la acci√≥n).
2. **La Actualizaci√≥n:** Multiplicas el error por la tasa de aprendizaje ($\alpha$) y lo a√±ades al valor $Q(s, a)$ antiguo.
ACTUALIZAR CON LO QUE HAGAMOS CON EL VALOR DE $\alpha$
    - Si $\alpha$ es grande, el agente aprende r√°pido pero puede ser inestable.
    - Si $\alpha$ es peque√±o, el aprendizaje es lento pero m√°s estable.

### Paso 7: Transici√≥n
El nuevo estado $s'$ se convierte en el estado actual $s$, y el proceso se repite desde el Paso 4 hasta que el episodio termina.

## 3. Optuna: Encontrando los Mejores Hiperpar√°metros
Los hiperpar√°metros ($\alpha, \gamma, \epsilon$ decay, $N_{bins}$) controlan el aprendizaje. Elegirlos manualmente es dif√≠cil. Optuna automatiza esto.
### Paso 8: La Funci√≥n Objetivo
Defines una funci√≥n (objective) que hace lo siguiente:
- **Pregunta a Optuna:** "Dame un valor para $\alpha$, dame un valor para $\gamma$, etc."
- **Entrena:** Ejecuta todo el proceso de Q-Learning (Pasos 1 a 7) con esos valores.
- **Eval√∫a:** Mide el rendimiento (ej., la recompensa media de los √∫ltimos 100 episodios).Informa a Optuna: Devuelve esa recompensa media.

### Paso 9: El Estudio de Optimizaci√≥n
Optuna ejecuta esta funci√≥n objetivo (ej., 100 veces)ACTUALIZAR CON LOS PASOS QUE HAGAMOS Y LA DESCRIPCION DEL NUESTRO. En cada prueba, utiliza algoritmos inteligentes (TPE, CMA-ES) para sugerir combinaciones de hiperpar√°metros que probablemente den mejores resultados bas√°ndose en las pruebas anteriores.

### Paso 10: El Resultado √ìptimo
Al finalizar, Optuna te dir√°: "La mejor combinaci√≥n de hiperpar√°metros es esta, y con ellos, el agente consigui√≥ esta recompensa media m√°xima."

Resumen del Flujo L√≥gico:
1. Optuna Sugiere Par√°metros ($\alpha, \gamma, N_{bins}$).
2. Q-Learning Inicializa la Q-Tabla usando $N_{bins}$.
3. Episodios: El agente interact√∫a y $\epsilon$ decae.
4. Actualizaci√≥n: Se usa $\alpha$ y $\gamma$ en la Ecuaci√≥n de Bellman.
5. Rendimiento: Se calcula la recompensa media.
6. Optuna Aprende: Optuna usa este resultado para elegir mejores par√°metros para la siguiente prueba.